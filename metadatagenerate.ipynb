{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm9lLfSVYjy2134doyv+Vk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VamshiPoojari/MetaDataGenrate/blob/main/metadatagenerate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly i would like to start by setting up an environment for reading text from PDFs and Word docs, Processing and analyzing that text with NLP techniques and Building and hosting a web interface (with Flask + ngrok) for user interaction"
      ],
      "metadata": {
        "id": "T7y9yroSaZ6h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P37e5mraV2ux",
        "outputId": "db0b698b-5cd6-498c-90d4-dbd1633e8f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, pypdfium2, pyngrok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pdfminer.six, nvidia-cusolver-cu12, flask-ngrok, pdfplumber\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed flask-ngrok-0.0.25 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pdfminer.six-20250506 pdfplumber-0.11.7 pyngrok-7.2.11 pypdfium2-4.30.1 python-docx-1.2.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber python-docx nltk spacy transformers sentence-transformers flask flask-ngrok pyngrok\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import docx\n",
        "import nltk\n",
        "import spacy\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from flask import Flask, request, render_template_string, jsonify\n",
        "import os\n",
        "from werkzeug.utils import secure_filename\n"
      ],
      "metadata": {
        "id": "WQk_QDhCW7d2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block imports all the necessary libraries for a document processing and NLP web application. It includes tools to:\n",
        "\n",
        "\n",
        "*   Extract text from PDFs (pdfplumber) and Word documents (docx)\n",
        "\n",
        "*   Analyze and process language using NLP libraries like nltk, spacy, transformers, and sentence-transformers\n",
        "\n",
        "*   Use deep learning models for tasks like summarization or embeddings (transformers, SentenceTransformer, torch)\n",
        "\n",
        "*   Handle data structures and formatting (re, json, datetime, pandas)\n",
        "\n",
        "*   Build a Flask web server to handle file uploads, form inputs, and return processed results via HTML or JSON (flask, werkzeug)\n",
        "\n",
        "*   Manage file paths and storage securely (os, secure_filename)\n"
      ],
      "metadata": {
        "id": "57s_sb3VawLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('punkt', quiet=True)\n",
        " nltk.download('stopwords', quiet=True)\n",
        " nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        " print(\"downloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlmyoQ_pahsG",
        "outputId": "e45b1663-5a81-430a-ac06-f49b5e8b6146"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code downloads three essential language models from NLTK: punkt for breaking text into sentences and words, stopwords for filtering out common English words that don’t carry much meaning (like \"the\" or \"is\"), and averaged_perceptron_tagger for identifying parts of speech in text. The quiet=True argument hides extra output during downloads, and once all resources are ready, it prints “downloaded” to confirm completion."
      ],
      "metadata": {
        "id": "YoJDwnL3c1sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "TAtssdSpa6p6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line loads English language model into object nlp using spaCy"
      ],
      "metadata": {
        "id": "Y9DViNUnc226"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#just a test\n",
        "doc = nlp(\"This is a test sentence\")\n",
        "print(len(doc))\n",
        "current_time = datetime.now()\n",
        "print(current_time.strftime('%Y-%m-%d %H:%M:%S'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LuEJ9WjbFeW",
        "outputId": "1bb9a8c1-d876-49ba-a2fe-962f7d05588e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "2025-06-24 15:15:09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract pillow pdf2image opencv-python --quiet\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path\n",
        "import cv2\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "nCksXed4bknB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this block of code imports necessary libraries for performing OCR (Optical Character Recognition) on images and PDFs. Pillow (PIL) handles image manipulation, while pdf2image converts PDF pages into images so they can be processed by OCR."
      ],
      "metadata": {
        "id": "AJ0Y27JuefIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DocumentExtractor:\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def extract_from_pdf(self, pdf_path):\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                print(f\" lenght: { len(pdf.pages) }\")\n",
        "                for page_num, page in enumerate(pdf.pages, 1):\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "                        print(f\"  Page {page_num} processed\")\n",
        "                print(f\" PDF extraction complete: {len(text)} characters extracted\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error extracting PDF: {e}\")\n",
        "            return \"\"\n",
        "        return text\n",
        "\n",
        "    def extract_from_pdf_with_ocr(self, pdf_path):\n",
        "\n",
        "        regular_text = self.extract_from_pdf(pdf_path)\n",
        "\n",
        "        # Check if we got sufficient text\n",
        "        word_count = len(regular_text.split()) if regular_text else 0\n",
        "\n",
        "        if word_count > 30:\n",
        "            print(f\" PDF extraction successful: {word_count} words\")\n",
        "            return regular_text\n",
        "        else:\n",
        "            print(f\" Limited text found ({word_count} words), switching to OCR...\")\n",
        "            return self._extract_with_ocr(pdf_path)\n",
        "\n",
        "    def _extract_with_ocr(self, pdf_path):\n",
        "\n",
        "        try:\n",
        "            # limit to first 3 pages for performance\n",
        "            pages = convert_from_path(pdf_path, dpi=200, first_page=1, last_page=3)\n",
        "\n",
        "            ocr_text = \"\"\n",
        "            for page_num, page_image in enumerate(pages, 1):\n",
        "                print(f\"  Processing page {page_num} with OCR...\")\n",
        "\n",
        "                # Extract text using OCR\n",
        "                page_text = pytesseract.image_to_string(page_image, lang='eng', config='--psm 6')\n",
        "\n",
        "                if page_text.strip():\n",
        "                    ocr_text += f\"{page_text}\\n\"\n",
        "                    print(f\"  Page {page_num}: {len(page_text.split())} words extracted\")\n",
        "                else:\n",
        "                    print(f\" Page {page_num}: No text found\")\n",
        "\n",
        "            if ocr_text.strip():\n",
        "                total_words = len(ocr_text.split())\n",
        "                print(f\" OCR extraction complete: {total_words} words extracted\")\n",
        "                return ocr_text\n",
        "            else:\n",
        "                print(\" no text\")\n",
        "                return \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" OCR extraction error: {e}\")\n",
        "            return self.extract_from_pdf(pdf_path)  # Fallback to regular method\n",
        "\n",
        "    def extract_from_docx(self, docx_path):\n",
        "\n",
        "        text = \"\"\n",
        "        try:\n",
        "            doc = docx.Document(docx_path)\n",
        "            paragraph_count = len(doc.paragraphs)\n",
        "            print(f\"there are {paragraph_count} paragraphs\")\n",
        "\n",
        "            for i, paragraph in enumerate(doc.paragraphs, 1):\n",
        "                if paragraph.text.strip():  # Only add non-empty paragraphs\n",
        "                    text += paragraph.text + \"\\n\"\n",
        "\n",
        "            print(f\"extraction complete: {len(text)} characters extracted\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error extracting Word document: {e}\")\n",
        "            return \"\"\n",
        "        return text\n",
        "\n",
        "    def extract_from_txt(self, txt_path):\n",
        "        try:\n",
        "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "            print(f\" Text file extraction complete: {len(text)} characters extracted\")\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\" Error extracting text file: {e}\")\n",
        "            return \"\"\n",
        "    # extract_content with ocr and extract content have same code\n",
        "    def extract_content_with_ocr(self, file_path):\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\" File not found: {file_path}\")\n",
        "            return \"\"\n",
        "\n",
        "        file_extension = file_path.lower().split('.')[-1]\n",
        "        file_size = os.path.getsize(file_path)\n",
        "\n",
        "        print(f\" File: {os.path.basename(file_path)}\")\n",
        "        print(f\"   File type: {file_extension.upper()}\")\n",
        "        print(f\"   File size: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
        "\n",
        "        # Choose the right extraction method\n",
        "        if file_extension == 'pdf':\n",
        "            return self.extract_from_pdf_with_ocr(file_path)\n",
        "        elif file_extension == 'docx':\n",
        "            return self.extract_from_docx(file_path)\n",
        "        elif file_extension == 'txt':\n",
        "            return self.extract_from_txt(file_path)\n",
        "        else:\n",
        "            print(f\" Unsupported file format: {file_extension}\")\n",
        "            print(\"   Supported formats: PDF, DOCX, TXT\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_content(self, file_path):\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\" File not found: {file_path}\")\n",
        "            return \"\"\n",
        "\n",
        "        # Get file extension to determine file type\n",
        "        file_extension = file_path.lower().split('.')[-1]\n",
        "        file_size = os.path.getsize(file_path)\n",
        "\n",
        "        print(f\"File: {os.path.basename(file_path)}\")\n",
        "        print(f\"   File type: {file_extension.upper()}\")\n",
        "        print(f\"  File size: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
        "\n",
        "        # Choose the right extraction method with OCR support\n",
        "        if file_extension == 'pdf':\n",
        "            return self.extract_from_pdf_with_ocr(file_path)\n",
        "        elif file_extension == 'docx':\n",
        "            return self.extract_from_docx(file_path)\n",
        "        elif file_extension == 'txt':\n",
        "            return self.extract_from_txt(file_path)\n",
        "        else:\n",
        "            print(f\" Unsupported file format: {file_extension}\")\n",
        "            print(\"   Supported formats: PDF, DOCX, TXT\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "extractor = DocumentExtractor()\n",
        "\n",
        "\n",
        "test_content = \"\"\"\n",
        "Sample Document for Testing\n",
        "\n",
        "This is a sample document created to test our automated metadata generation system with OCR support.\n",
        "\n",
        "Key Information:\n",
        "- Document Type: Test Document\n",
        "- Created by: AI System\n",
        "- Date: June 2025\n",
        "- Purpose: Testing document extraction capabilities with OCR\n",
        "\n",
        "The system should be able to extract this text and analyze it for metadata generation.\n",
        "This includes identifying key concepts, generating summaries, and extracting relevant information.\n",
        "\n",
        "OCR Integration: The system now supports both digital and scanned documents automatically.\n",
        "\n",
        "Thank you for testing our enhanced system!\n",
        "\"\"\"\n",
        "\n",
        "test_file_path = 'test_content.txt'\n",
        "with open(test_file_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(test_content)\n",
        "\n",
        "print(f\"\\n sample: {test_file_path}\")\n",
        "\n",
        "extracted_text = extractor.extract_content(test_file_path)\n",
        "\n",
        "if extracted_text:\n",
        "    print(f\"\\n Test Results:\")\n",
        "    print(f\"    Total characters: {len(extracted_text)}\")\n",
        "    print(f\"    Total words: {len(extracted_text.split())}\")\n",
        "    print(f\"    First 100 characters: {extracted_text[:100]}...\")\n",
        "\n",
        "else:\n",
        "    print(\" Extraction test failed\")\n",
        "\n",
        "\n",
        "os.remove(test_file_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEYx4IZDjivJ",
        "outputId": "d43a5078-9a65-4221-a02d-966fbfe0a4ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " sample: test_content.txt\n",
            "File: test_content.txt\n",
            "   File type: TXT\n",
            "  File size: 606 bytes (0.6 KB)\n",
            " Text file extraction complete: 606 characters extracted\n",
            "\n",
            " Test Results:\n",
            "    Total characters: 606\n",
            "    Total words: 89\n",
            "    First 100 characters: \n",
            "Sample Document for Testing\n",
            "\n",
            "This is a sample document created to test our automated metadata gener...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script defines a DocumentExtractor class designed to extract text from various document formats—PDFs (with or without OCR), Word (.docx), and plain text (.txt) files. It uses libraries like pdfplumber for regular PDFs, pytesseract and pdf2image for OCR-based extraction from scanned PDFs, and python-docx for Word files. When a file is passed to extract_content or extract_content_with_ocr, the class identifies the file type and chooses the appropriate method to extract readable text. At the end of the script, a sample .txt file is created and processed using this extractor. The script prints out metadata such as word count and a preview of the extracted text. Finally, it cleans up by deleting the test file."
      ],
      "metadata": {
        "id": "O1vNWmGGg5OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "class SemanticAnalyzer:\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "        from nltk.corpus import stopwords\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def extract_key_entities(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        entities = []\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'MONEY', 'DATE', 'EVENT']:\n",
        "                entities.append({\n",
        "                    'text': ent.text.strip(),\n",
        "                    'label': ent.label_,\n",
        "                    'description': spacy.explain(ent.label_)\n",
        "                })\n",
        "\n",
        "        unique_entities = []\n",
        "        seen = set()\n",
        "        for entity in entities:\n",
        "            if entity['text'].lower() not in seen:\n",
        "                unique_entities.append(entity)\n",
        "                seen.add(entity['text'].lower())\n",
        "\n",
        "        return unique_entities[:10]\n",
        "\n",
        "    def generate_summary(self, text, max_sentences=3):\n",
        "        if len(text) < 100:\n",
        "            return text\n",
        "\n",
        "        doc = self.nlp(text)\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 20]\n",
        "\n",
        "        if len(sentences) <= max_sentences:\n",
        "            return ' '.join(sentences)\n",
        "\n",
        "        word_frequencies = {}\n",
        "        for word in doc:\n",
        "            if not word.is_stop and not word.is_punct and len(word.text) > 2:\n",
        "                word_frequencies[word.lemma_.lower()] = word_frequencies.get(word.lemma_.lower(), 0) + 1\n",
        "\n",
        "        sentence_scores = {}\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence_doc = self.nlp(sentence)\n",
        "            score = 0\n",
        "            word_count = 0\n",
        "\n",
        "            for word in sentence_doc:\n",
        "                if not word.is_stop and not word.is_punct and len(word.text) > 2:\n",
        "                    score += word_frequencies.get(word.lemma_.lower(), 0)\n",
        "                    word_count += 1\n",
        "\n",
        "            if word_count > 0:\n",
        "                sentence_scores[i] = (score / word_count) + (len(sentences) - i) * 0.1\n",
        "\n",
        "        top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:max_sentences]\n",
        "        top_sentences = sorted(top_sentences, key=lambda x: x[0])  # Sort by original order\n",
        "\n",
        "        summary_sentences = [sentences[i] for i, score in top_sentences]\n",
        "        return ' '.join(summary_sentences)\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic',\n",
        "                         'positive', 'success', 'successful', 'effective', 'efficient', 'improve',\n",
        "                         'better', 'best', 'increase', 'growth', 'benefit', 'advantage'}\n",
        "\n",
        "        negative_words = {'bad', 'terrible', 'awful', 'horrible', 'negative', 'fail', 'failure',\n",
        "                         'poor', 'worse', 'worst', 'decrease', 'decline', 'problem', 'issue',\n",
        "                         'difficult', 'challenge', 'risk', 'threat', 'disadvantage'}\n",
        "\n",
        "        doc = self.nlp(text.lower())\n",
        "        positive_count = 0\n",
        "        negative_count = 0\n",
        "        total_words = 0\n",
        "\n",
        "        for token in doc:\n",
        "            if not token.is_stop and not token.is_punct and len(token.text) > 2:\n",
        "                total_words += 1\n",
        "                if token.lemma_ in positive_words:\n",
        "                    positive_count += 1\n",
        "                elif token.lemma_ in negative_words:\n",
        "                    negative_count += 1\n",
        "\n",
        "        if positive_count > negative_count:\n",
        "            sentiment = 'POSITIVE'\n",
        "            confidence = min(0.9, 0.5 + (positive_count - negative_count) / total_words * 10)\n",
        "        elif negative_count > positive_count:\n",
        "            sentiment = 'NEGATIVE'\n",
        "            confidence = min(0.9, 0.5 + (negative_count - positive_count) / total_words * 10)\n",
        "        else:\n",
        "            sentiment = 'NEUTRAL'\n",
        "            confidence = 0.5\n",
        "\n",
        "        return {\n",
        "            'sentiment': sentiment,\n",
        "            'confidence': round(confidence, 2)\n",
        "        }\n",
        "\n",
        "    def extract_keywords(self, text, top_k=10):\n",
        "        \"\"\"Extract the most important keywords from the document\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        keywords = []\n",
        "        for token in doc:\n",
        "            if (not token.is_stop and\n",
        "                not token.is_punct and\n",
        "                len(token.text) > 2 and\n",
        "                token.pos_ in ['NOUN', 'ADJ', 'PROPN'] and\n",
        "                token.lemma_.lower() not in self.stop_words):\n",
        "                keywords.append(token.lemma_.lower())\n",
        "\n",
        "        # Count frequency and return top keywords\n",
        "        keyword_freq = Counter(keywords)\n",
        "        return [word for word, freq in keyword_freq.most_common(top_k)]\n",
        "\n",
        "\n",
        "\n",
        "analyzer = SemanticAnalyzer()\n",
        "\n",
        "sample_text = \"\"\"\n",
        "Artificial Intelligence and Machine Learning Report\n",
        "\n",
        "This comprehensive document provides an overview of artificial intelligence (AI) and machine learning (ML)\n",
        "technologies in 2024. The report was prepared by Dr. Sarah Johnson at Stanford University.\n",
        "\n",
        "Key findings include significant developments in the field:\n",
        "- AI adoption has increased by 40% in enterprise environments, showing positive growth\n",
        "- Machine learning models are becoming more efficient and effective\n",
        "- Natural language processing shows promising results across various applications\n",
        "\n",
        "The research was conducted between January and March 2024, involving 500 companies\n",
        "across various industries including technology, healthcare, and finance. The study cost $2.5 million.\n",
        "\n",
        "Major recommendations for organizations:\n",
        "1. Invest in comprehensive AI training for employees\n",
        "2. Implement gradual AI integration strategies\n",
        "3. Focus on improving data quality and management\n",
        "\n",
        "The findings demonstrate excellent progress in AI capabilities. However, some challenges remain\n",
        "in terms of implementation costs and training requirements.\n",
        "\n",
        "For more information about this groundbreaking research, contact the research team at Stanford University.\n",
        "\"\"\"\n",
        "\n",
        "print(f\" sample document has {len(sample_text)} characters\")\n",
        "\n",
        "# Test all analyzer functions\n",
        "entities = analyzer.extract_key_entities(sample_text)\n",
        "summary = analyzer.generate_summary(sample_text)\n",
        "sentiment = analyzer.analyze_sentiment(sample_text)\n",
        "keywords = analyzer.extract_keywords(sample_text)\n",
        "\n",
        "\n",
        "print(f\"\\n Extracted Entities {len(entities)} found:\")\n",
        "for entity in entities:\n",
        "    print(f\"   • {entity['text']} ({entity['label']}) - {entity['description']}\")\n",
        "\n",
        "print(f\"\\n Generated Summary:\")\n",
        "print(f\"   {summary}\")\n",
        "\n",
        "print(f\"\\n Sentiment Analysis:\")\n",
        "print(f\"   Sentiment: {sentiment['sentiment']}\")\n",
        "print(f\"   Confidence: {sentiment['confidence']*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n Keywords ({len(keywords)} found):\")\n",
        "print(f\"   {', '.join(keywords)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0rlom5YffUp",
        "outputId": "82787274-66da-42a9-f581-2124ab576754"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " sample document has 1193 characters\n",
            "\n",
            " Extracted Entities 8 found:\n",
            "   • Artificial Intelligence and Machine Learning Report (ORG) - Companies, agencies, institutions, etc.\n",
            "   • ML (ORG) - Companies, agencies, institutions, etc.\n",
            "   • 2024 (DATE) - Absolute or relative dates or periods\n",
            "   • Sarah Johnson (PERSON) - People, including fictional\n",
            "   • Stanford University (ORG) - Companies, agencies, institutions, etc.\n",
            "   • between January and March 2024 (DATE) - Absolute or relative dates or periods\n",
            "   • $2.5 million (MONEY) - Monetary values, including unit\n",
            "   • AI (ORG) - Companies, agencies, institutions, etc.\n",
            "\n",
            " Generated Summary:\n",
            "   Artificial Intelligence and Machine Learning Report\n",
            "\n",
            "This comprehensive document provides an overview of artificial intelligence (AI) and machine learning (ML)\n",
            "technologies in 2024. The report was prepared by Dr. Sarah Johnson at Stanford University. Key findings include significant developments in the field:\n",
            "- AI adoption has increased by 40% in enterprise environments, showing positive growth\n",
            "- Machine learning models are becoming more efficient and effective\n",
            "- Natural language processing shows promising results across various applications\n",
            "\n",
            " Sentiment Analysis:\n",
            "   Sentiment: POSITIVE\n",
            "   Confidence: 90.0%\n",
            "\n",
            " Keywords (10 found):\n",
            "   machine, learning, research, artificial, intelligence, report, comprehensive, technology, stanford, university\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Analyser\n",
        "\n",
        "This class analyzes the meaning and content of documents using spaCy and NLTK.\n",
        "It can extract entities, keywords, create summaries, and analyze sentiment.\n",
        "\n",
        "*    It includes a method to extract named entities such as people, organizations, locations, and dates, ensuring duplicates are removed.\n",
        "\n",
        "*   Another method generates a summary by scoring sentences based on word\n",
        "frequency and selecting the most relevant ones.\n",
        "*   Sentiment analysis is done using predefined lists of positive and negative words, computing a basic confidence score to classify the text as positive, negative, or neutral.\n",
        "\n",
        "*   The script also includes a keyword extraction method that identifies and returns the most frequent nouns, adjectives, and proper nouns after filtering out stopwords and punctuation.\n",
        "\n",
        "To test these features, a sample text about artificial intelligence and machine learning is analyzed. The script prints out key entities, a summary, sentiment with confidence, and important keywords, demonstrating that the semantic analysis tools are functioning as expected.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VkaS1c_bxpa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetadataGenerator:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.extractor = DocumentExtractor()\n",
        "        self.analyzer = SemanticAnalyzer()\n",
        "\n",
        "    def generate_metadata(self, file_path, filename):\n",
        "\n",
        "        content = self.extractor.extract_content(file_path)\n",
        "\n",
        "        if not content or len(content) < 10:\n",
        "            return {\n",
        "                \"error\": \"Could not extract sufficient content from file\",\n",
        "                \"filename\": filename,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "        print(f\"   Content extracted: {len(content)} characters\")\n",
        "\n",
        "        try:\n",
        "            file_stats = os.stat(file_path)\n",
        "            file_info = {\n",
        "                \"filename\": filename,\n",
        "                \"file_size_bytes\": file_stats.st_size,\n",
        "                \"file_size_mb\": round(file_stats.st_size / (1024*1024), 3),\n",
        "                \"file_size_kb\": round(file_stats.st_size / 1024, 1),\n",
        "                \"created_date\": datetime.fromtimestamp(file_stats.st_ctime).isoformat(),\n",
        "                \"modified_date\": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),\n",
        "                \"file_type\": filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'\n",
        "            }\n",
        "            print(f\"   File info collected: {file_info['file_size_kb']} KB, {file_info['file_type']} format\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Could not get file stats: {e}\")\n",
        "            file_info = {\"filename\": filename, \"error\": \"Could not access file stats\"}\n",
        "\n",
        "        words = content.split()\n",
        "        sentences = content.split('.')\n",
        "        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        content_stats = {\n",
        "            \"word_count\": len(words),\n",
        "            \"character_count\": len(content),\n",
        "            \"character_count_no_spaces\": len(content.replace(' ', '')),\n",
        "            \"sentence_count\": len([s for s in sentences if s.strip()]),\n",
        "            \"paragraph_count\": len(paragraphs),\n",
        "            \"average_words_per_sentence\": round(len(words) / max(1, len([s for s in sentences if s.strip()])), 1),\n",
        "            \"estimated_reading_time_minutes\": round(len(words) / 200, 1),  # 200 words per minute average\n",
        "            \"readability_score\": self._calculate_readability(content)\n",
        "        }\n",
        "        print(f\"   Content stats: {content_stats['word_count']} words, {content_stats['sentence_count']} sentences\")\n",
        "\n",
        "        analysis_content = content[:3000] if len(content) > 3000 else content\n",
        "\n",
        "        entities = self.analyzer.extract_key_entities(analysis_content)\n",
        "        summary = self.analyzer.generate_summary(analysis_content)\n",
        "        sentiment = self.analyzer.analyze_sentiment(analysis_content)\n",
        "        keywords = self.analyzer.extract_keywords(analysis_content)\n",
        "\n",
        "        print(f\"Semantic analysis complete: {len(entities)} entities, {len(keywords)} keywords\")\n",
        "\n",
        "        # Calculate confidence score\n",
        "        confidence_score = self._calculate_confidence_score(content, entities, keywords, content_stats)\n",
        "\n",
        "        metadata = {\n",
        "            \"document_metadata\": {\n",
        "                \"generation_info\": {\n",
        "                    \"generated_at\": datetime.now().isoformat(),\n",
        "                    \"generator_version\": \"1.0.0\",\n",
        "                    \"processing_time_seconds\": \"< 1\",\n",
        "                    \"confidence_score\": confidence_score,\n",
        "                    \"analysis_completeness\": \"Full\" if len(content) <= 3000 else \"Partial (first 3000 chars)\"\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"file_information\": file_info,\n",
        "\n",
        "            \"content_analysis\": content_stats,\n",
        "\n",
        "            \"semantic_analysis\": {\n",
        "                \"document_summary\": summary,\n",
        "                \"primary_keywords\": keywords,\n",
        "                \"sentiment_analysis\": sentiment,\n",
        "                \"named_entities\": entities,\n",
        "                \"content_type\": self._classify_content_type(content, entities, keywords),\n",
        "                \"main_topics\": self._extract_main_topics(keywords, entities)\n",
        "            },\n",
        "\n",
        "            \"quality_indicators\": {\n",
        "                \"content_richness\": \"High\" if len(words) > 500 else \"Medium\" if len(words) > 100 else \"Low\",\n",
        "                \"entity_density\": round(len(entities) / max(1, len(words)) * 100, 2),\n",
        "                \"keyword_diversity\": len(set(keywords)),\n",
        "                \"structural_complexity\": \"High\" if content_stats['paragraph_count'] > 10 else \"Medium\" if content_stats['paragraph_count'] > 3 else \"Low\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(f\"   Confidence Score: {confidence_score*100:.1f}%\")\n",
        "        print(f\"   Content Type: {metadata['semantic_analysis']['content_type']}\")\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def _calculate_confidence_score(self, content, entities, keywords, content_stats):\n",
        "        score = 0.3\n",
        "\n",
        "        # Content length factor\n",
        "        word_count = content_stats['word_count']\n",
        "        if word_count > 500:\n",
        "            score += 0.3\n",
        "        elif word_count > 200:\n",
        "            score += 0.2\n",
        "        elif word_count > 50:\n",
        "            score += 0.1\n",
        "\n",
        "        # Entity richness factor\n",
        "        if len(entities) > 5:\n",
        "            score += 0.2\n",
        "        elif len(entities) > 2:\n",
        "            score += 0.15\n",
        "        elif len(entities) > 0:\n",
        "            score += 0.1\n",
        "\n",
        "        # Keyword diversity factor\n",
        "        if len(keywords) > 8:\n",
        "            score += 0.15\n",
        "        elif len(keywords) > 4:\n",
        "            score += 0.1\n",
        "        elif len(keywords) > 0:\n",
        "            score += 0.05\n",
        "\n",
        "        # Structure factor\n",
        "        if content_stats['paragraph_count'] > 3:\n",
        "            score += 0.1\n",
        "\n",
        "        return min(1.0, round(score, 2))\n",
        "\n",
        "    def _calculate_readability(self, text):\n",
        "\n",
        "        words = len(text.split())\n",
        "        sentences = len([s for s in text.split('.') if s.strip()])\n",
        "        if sentences == 0:\n",
        "            return \"Unknown\"\n",
        "\n",
        "        avg_words_per_sentence = words / sentences\n",
        "        if avg_words_per_sentence < 15:\n",
        "            return \"Easy\"\n",
        "        elif avg_words_per_sentence < 25:\n",
        "            return \"Medium\"\n",
        "        else:\n",
        "            return \"Complex\"\n",
        "\n",
        "    def _classify_content_type(self, content, entities, keywords):\n",
        "\n",
        "        content_lower = content.lower()\n",
        "\n",
        "        # Check for what sort of content it is whether it is a academic content ,business, technical or legal content\n",
        "        academic_terms = ['research', 'study', 'analysis', 'university', 'findings', 'methodology']\n",
        "        if any(term in content_lower for term in academic_terms):\n",
        "            return \"Academic/Research\"\n",
        "\n",
        "        business_terms = ['company', 'business', 'market', 'revenue', 'strategy', 'customers']\n",
        "        if any(term in content_lower for term in business_terms):\n",
        "            return \"Business\"\n",
        "\n",
        "        tech_terms = ['system', 'technology', 'software', 'data', 'algorithm', 'technical']\n",
        "        if any(term in content_lower for term in tech_terms):\n",
        "            return \"Technical\"\n",
        "\n",
        "        legal_terms = ['law', 'legal', 'contract', 'agreement', 'terms', 'conditions']\n",
        "        if any(term in content_lower for term in legal_terms):\n",
        "            return \"Legal\"\n",
        "\n",
        "        return \"General\"\n",
        "\n",
        "    def _extract_main_topics(self, keywords, entities):\n",
        "\n",
        "        all_terms = keywords + [entity['text'].lower() for entity in entities]\n",
        "\n",
        "\n",
        "        topics = []\n",
        "        if any('ai' in term or 'artificial' in term or 'machine' in term for term in all_terms):\n",
        "            topics.append(\"Artificial Intelligence\")\n",
        "        if any('business' in term or 'company' in term or 'market' in term for term in all_terms):\n",
        "            topics.append(\"Business\")\n",
        "        if any('research' in term or 'study' in term or 'analysis' in term for term in all_terms):\n",
        "            topics.append(\"Research\")\n",
        "        if any('technology' in term or 'technical' in term or 'system' in term for term in all_terms):\n",
        "            topics.append(\"Technology\")\n",
        "\n",
        "        return topics if topics else [\"General\"]\n",
        "\n",
        "#created metadata generator\n",
        "generator = MetadataGenerator()\n",
        "\n",
        "test_document = \"\"\"\n",
        "Artificial Intelligence in Healthcare: A Comprehensive Study\n",
        "\n",
        "Executive Summary\n",
        "\n",
        "This research report examines the implementation of artificial intelligence (AI) technologies\n",
        "in healthcare systems across North America. The study was conducted by Dr. Emily Chen at\n",
        "Massachusetts General Hospital in collaboration with Stanford University.\n",
        "\n",
        "Background and Methodology\n",
        "\n",
        "The research team analyzed data from 150 hospitals between January 2024 and May 2024.\n",
        "The study involved a budget of $1.2 million and included interviews with 500 healthcare\n",
        "professionals across various specialties.\n",
        "\n",
        "Key Findings\n",
        "\n",
        "Our analysis reveals significant improvements in patient outcomes through AI integration:\n",
        "\n",
        "1. Diagnostic accuracy increased by 23% when AI systems were used alongside traditional methods\n",
        "2. Patient wait times decreased by an average of 35 minutes\n",
        "3. Treatment costs were reduced by 18% on average\n",
        "4. Staff satisfaction improved due to reduced administrative burden\n",
        "\n",
        "The most successful implementations were found in radiology, pathology, and emergency medicine.\n",
        "Dr. Sarah Johnson, Chief of Radiology at Boston Medical Center, noted: \"AI has revolutionized\n",
        "our diagnostic capabilities while allowing us to focus more on patient care.\"\n",
        "\n",
        "Challenges and Limitations\n",
        "\n",
        "Despite positive outcomes, several challenges were identified:\n",
        "- Initial implementation costs averaging $250,000 per department\n",
        "- Training requirements for medical staff\n",
        "- Data privacy and security concerns\n",
        "- Regulatory compliance issues\n",
        "\n",
        "Future Recommendations\n",
        "\n",
        "Based on our findings, we recommend:\n",
        "1. Gradual implementation starting with high-impact areas\n",
        "2. Comprehensive staff training programs\n",
        "3. Investment in robust cybersecurity measures\n",
        "4. Collaboration with technology vendors for customized solutions\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The integration of AI in healthcare shows tremendous promise for improving patient outcomes\n",
        "and operational efficiency. However, successful implementation requires careful planning,\n",
        "adequate resources, and strong leadership commitment.\n",
        "\n",
        "This report was prepared by the Healthcare Innovation Research Team at Massachusetts General Hospital.\n",
        "For additional information, contact Dr. Emily Chen at echen@mgh.harvard.edu.\n",
        "\"\"\"\n",
        "\n",
        "test_file_path = '/tmp/comprehensive_test.txt'\n",
        "with open(test_file_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(test_document)\n",
        "\n",
        "print(f\" Created comprehensive test document ({len(test_document)} characters)\")\n",
        "\n",
        "metadata = generator.generate_metadata(test_file_path, 'comprehensive_test.txt')\n",
        "\n",
        "print(f\"\\n RESULTS\")\n",
        "\n",
        "if 'error' not in metadata:\n",
        "    print(f\" FILE: {metadata['file_information']['filename']}\")\n",
        "    print(f\" SIZE: {metadata['file_information']['file_size_kb']} KB\")\n",
        "    print(f\" WORDS: {metadata['content_analysis']['word_count']}\")\n",
        "    print(f\" READING TIME: {metadata['content_analysis']['estimated_reading_time_minutes']} minutes\")\n",
        "    print(f\" CONTENT TYPE: {metadata['semantic_analysis']['content_type']}\")\n",
        "    print(f\" SENTIMENT: {metadata['semantic_analysis']['sentiment_analysis']['sentiment']}\")\n",
        "    print(f\" CONFIDENCE: {metadata['document_metadata']['generation_info']['confidence_score']*100:.1f}%\")\n",
        "\n",
        "    print(f\"\\n SUMMARY:\")\n",
        "    print(f\"   {metadata['semantic_analysis']['document_summary'][:200]}...\")\n",
        "\n",
        "    print(f\"\\n TOP KEYWORDS:\")\n",
        "    print(f\"   {', '.join(metadata['semantic_analysis']['primary_keywords'][:8])}\")\n",
        "\n",
        "    print(f\"\\n ENTITIES FOUND:\")\n",
        "    for entity in metadata['semantic_analysis']['named_entities'][:5]:\n",
        "        print(f\"   • {entity['text']} ({entity['label']})\")\n",
        "\n",
        "else:\n",
        "    print(f\" Error: {metadata['error']}\")\n",
        "\n",
        "os.remove(test_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knv-vjVPHABJ",
        "outputId": "342903be-29e2-4030-c61f-20606e982377"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Created comprehensive test document (2204 characters)\n",
            "File: comprehensive_test.txt\n",
            "   File type: TXT\n",
            "  File size: 2,204 bytes (2.2 KB)\n",
            " Text file extraction complete: 2204 characters extracted\n",
            "   Content extracted: 2204 characters\n",
            "   File info collected: 2.2 KB, TXT format\n",
            "   Content stats: 292 words, 24 sentences\n",
            "Semantic analysis complete: 10 entities, 10 keywords\n",
            "   Confidence Score: 95.0%\n",
            "   Content Type: Academic/Research\n",
            "\n",
            " RESULTS\n",
            " FILE: comprehensive_test.txt\n",
            " SIZE: 2.2 KB\n",
            " WORDS: 292\n",
            " READING TIME: 1.5 minutes\n",
            " CONTENT TYPE: Academic/Research\n",
            " SENTIMENT: POSITIVE\n",
            " CONFIDENCE: 95.0%\n",
            "\n",
            " SUMMARY:\n",
            "   Artificial Intelligence in Healthcare: A Comprehensive Study\n",
            "\n",
            "Executive Summary\n",
            "\n",
            "This research report examines the implementation of artificial intelligence (AI) technologies\n",
            "in healthcare systems acr...\n",
            "\n",
            " TOP KEYWORDS:\n",
            "   healthcare, implementation, patient, study, research, dr., hospital, outcome\n",
            "\n",
            " ENTITIES FOUND:\n",
            "   • Artificial Intelligence (ORG)\n",
            "   • Healthcare (ORG)\n",
            "   • Emily Chen (PERSON)\n",
            "   • Massachusetts General Hospital (ORG)\n",
            "   • Stanford University (ORG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   This engine combines document extraction and semantic analysis to generate comprehensive metadata for any document.\n",
        "   It collects basic file info (like size and creation date), content stats (word count, sentence count, readability), and performs semantic analysis to generate a summary, detect sentiment, extract keywords, and identify named entities like people and organizations. It also classifies the content type (e.g., academic, technical) and estimates a confidence score based on the document’s richness and structure."
      ],
      "metadata": {
        "id": "2ItlXRo50bmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EnhancedMetadataGenerator(MetadataGenerator):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.extractor = DocumentExtractor()\n",
        "        self.analyzer = SemanticAnalyzer()\n",
        "\n",
        "        self.extractor.extract_content = self.extractor.extract_content_with_ocr\n",
        "\n",
        "    def generate_metadata(self, file_path, filename):\n",
        "        print(f\"\\n Generating metadata with OCR support {filename}\")\n",
        "\n",
        "        metadata = super().generate_metadata(file_path, filename)\n",
        "\n",
        "        if 'file_information' in metadata:\n",
        "            metadata['file_information']['ocr_capable'] = \"Yes\"\n",
        "            metadata['file_information']['extraction_method'] = \"Regular + OCR Fallback\"\n",
        "\n",
        "        return metadata\n",
        "\n",
        "metadata_generator = EnhancedMetadataGenerator()\n",
        "\n",
        "print(\"all done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGA1IE1_K_j_",
        "outputId": "88d83683-1a2a-40be-b253-2fe70075aee7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, render_template_string, jsonify\n",
        "from werkzeug.utils import secure_filename\n",
        "import os\n",
        "from datetime import datetime\n",
        "import threading\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['UPLOAD_FOLDER'] = '/tmp/uploads'\n",
        "app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size\n",
        "app.secret_key = 'metadata_generator_secret_key'\n",
        "\n",
        "os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
        "\n",
        "\n",
        "HTML_TEMPLATE_OCR = '''\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title> AI Metadata Generator with OCR</title>\n",
        "    <style>\n",
        "        * { margin: 0; padding: 0; box-sizing: border-box; }\n",
        "        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; padding: 20px; }\n",
        "        .container { max-width: 1000px; margin: 0 auto; background: white; border-radius: 20px; box-shadow: 0 20px 40px rgba(0,0,0,0.1); }\n",
        "        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 40px; text-align: center; border-radius: 20px 20px 0 0; }\n",
        "        .header h1 { font-size: 2.5em; margin-bottom: 10px; font-weight: 700; }\n",
        "        .header p { font-size: 1.2em; opacity: 0.9; }\n",
        "        .ocr-badge { background: rgba(255,255,255,0.2); padding: 8px 16px; border-radius: 20px; display: inline-block; margin-top: 10px; font-size: 0.9em; }\n",
        "        .content { padding: 40px; }\n",
        "        .upload-section { background: #f8f9fa; border: 3px dashed #dee2e6; border-radius: 15px; padding: 40px; text-align: center; margin-bottom: 30px; transition: all 0.3s ease; }\n",
        "        .upload-section:hover { border-color: #667eea; background: #f0f4ff; }\n",
        "        .file-input-button { padding: 15px 30px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 10px; cursor: pointer; border: none; font-size: 1.1em; font-weight: 600; }\n",
        "        .file-input-button:hover {\n",
        "            transform: translateY(-2px);\n",
        "            box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);\n",
        "        }\n",
        "        .submit-btn { background: linear-gradient(135deg, #28a745 0%, #20c997 100%); color: white; padding: 15px 40px; border: none; border-radius: 10px; font-size: 1.1em; cursor: pointer; margin-top: 20px; }\n",
        "        .results { background: #f8f9fa; padding: 30px; border-radius: 15px; margin-top: 30px; border-left: 5px solid #667eea; }\n",
        "        .stat-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-top: 15px; }\n",
        "        .stat-item { background: white; padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 2px 10px rgba(0,0,0,0.05); }\n",
        "        .stat-value { font-size: 1.5em; font-weight: bold; color: #667eea; }\n",
        "        .stat-label { color: #6c757d; font-size: 0.9em; margin-top: 5px; }\n",
        "        .tag { display: inline-block; background: #e3f2fd; color: #1565c0; padding: 5px 12px; margin: 3px; border-radius: 20px; font-size: 0.9em; }\n",
        "        .summary-box { background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 10px; padding: 20px; margin: 15px 0; font-style: italic; }\n",
        "        .error-message { background: #f8d7da; color: #721c24; padding: 20px; border-radius: 10px; margin-top: 20px; }\n",
        "        .ocr-indicator { background: #d4edda; color: #155724; padding: 10px; border-radius: 5px; margin: 10px 0; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <div class=\"header\">\n",
        "            <h1> AI Metadata Generator</h1>\n",
        "            <p>Advanced document analysis with OCR support</p>\n",
        "            <div class=\"ocr-badge\"> OCR Enabled for Scanned Documents</div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"content\">\n",
        "            <form method=\"POST\" enctype=\"multipart/form-data\">\n",
        "                <div class=\"upload-section\">\n",
        "                    <input type=\"file\" name=\"file\" accept=\".pdf,.docx,.txt\" required style=\"margin-bottom: 15px;\">\n",
        "                    <p> Upload PDF, DOCX, or TXT files</p>\n",
        "                    <p style=\"font-size: 0.9em; color: #6c757d; margin-top: 10px;\">\n",
        "                         Maax Limit of size: 16 MB\n",
        "                    </p>\n",
        "                </div>\n",
        "                <button type=\"submit\" class=\"submit-btn\"> Generate Metadata with AI</button>\n",
        "            </form>\n",
        "\n",
        "            {% if metadata %}\n",
        "            <div class=\"results\">\n",
        "                <h2> AI-Generated Metadata</h2>\n",
        "\n",
        "                {% if metadata.file_information.get('ocr_capable') == 'Yes' %}\n",
        "                <div class=\"ocr-indicator\">\n",
        "                     <strong>OCR Capability:</strong> This system can process scanned documents automatically\n",
        "                </div>\n",
        "                {% endif %}\n",
        "\n",
        "                <div class=\"stat-grid\">\n",
        "                    <div class=\"stat-item\">\n",
        "                        <div class=\"stat-value\">{{ metadata.file_information.filename }}</div>\n",
        "                        <div class=\"stat-label\">Filename</div>\n",
        "                    </div>\n",
        "                    <div class=\"stat-item\">\n",
        "                        <div class=\"stat-value\">{{ metadata.content_analysis.word_count }}</div>\n",
        "                        <div class=\"stat-label\">Words Extracted</div>\n",
        "                    </div>\n",
        "                    <div class=\"stat-item\">\n",
        "                        <div class=\"stat-value\">{{ metadata.semantic_analysis.content_type }}</div>\n",
        "                        <div class=\"stat-label\">Content Type</div>\n",
        "                    </div>\n",
        "                    <div class=\"stat-item\">\n",
        "                        <div class=\"stat-value\">{{ (metadata.document_metadata.generation_info.confidence_score * 100)|round(1) }}%</div>\n",
        "                        <div class=\"stat-label\">AI Confidence</div>\n",
        "                    </div>\n",
        "                </div>\n",
        "\n",
        "                <div style=\"margin-top: 25px;\">\n",
        "                    <h3> AI Analysis Results</h3>\n",
        "                    <div class=\"summary-box\">\n",
        "                        <strong>Summary:</strong> {{ metadata.semantic_analysis.document_summary }}\n",
        "                    </div>\n",
        "\n",
        "                    <p><strong> Keywords:</strong></p>\n",
        "                    {% for keyword in metadata.semantic_analysis.primary_keywords %}\n",
        "                        <span class=\"tag\">{{ keyword }}</span>\n",
        "                    {% endfor %}\n",
        "\n",
        "                    <p style=\"margin-top: 15px;\"><strong> Sentiment:</strong>\n",
        "                        <span class=\"tag\">{{ metadata.semantic_analysis.sentiment_analysis.sentiment }}</span>\n",
        "                    </p>\n",
        "                </div>\n",
        "            </div>\n",
        "            {% endif %}\n",
        "\n",
        "            {% if error %}\n",
        "            <div class=\"error-message\">\n",
        "                <strong> Error:</strong> {{ error }}\n",
        "            </div>\n",
        "            {% endif %}\n",
        "        </div>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def upload_file():\n",
        "    if request.method == 'POST':\n",
        "        if 'file' not in request.files:\n",
        "            return render_template_string(HTML_TEMPLATE_OCR, error='No file selected')\n",
        "\n",
        "        file = request.files['file']\n",
        "        if file.filename == '':\n",
        "            return render_template_string(HTML_TEMPLATE_OCR, error='No file selected')\n",
        "\n",
        "        if file:\n",
        "            filename = secure_filename(file.filename)\n",
        "            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "\n",
        "            try:\n",
        "                file.save(file_path)\n",
        "                print(f\" Processing {filename} with OCR-enhanced system...\")\n",
        "\n",
        "                metadata = metadata_generator.generate_metadata(file_path, filename)\n",
        "\n",
        "                os.remove(file_path)\n",
        "\n",
        "                if 'error' in metadata:\n",
        "                    return render_template_string(HTML_TEMPLATE_OCR, error=metadata['error'])\n",
        "\n",
        "                print(f\" Successfully processed {filename} \")\n",
        "                return render_template_string(HTML_TEMPLATE_OCR, metadata=metadata)\n",
        "\n",
        "            except Exception as e:\n",
        "                if os.path.exists(file_path):\n",
        "                    os.remove(file_path)\n",
        "                return render_template_string(HTML_TEMPLATE_OCR, error=f'Error: {str(e)}')\n",
        "\n",
        "    return render_template_string(HTML_TEMPLATE_OCR)\n"
      ],
      "metadata": {
        "id": "5Iqv77L-NsGl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Flask-based web application allows users to upload documents (PDF, DOCX, or TXT) and receive detailed AI-generated metadata. It supports both digital and scanned documents using OCR, making it useful for extracting information from a wide range of files. The system performs semantic analysis to generate summaries, keywords, sentiment, and more — all through a clean, user-friendly interface.\n",
        "\n",
        "*   **Web App Setup**: Uses Flask to handle uploads of files up to 16MB, with a secure and configurable server environment.\n",
        "*   **File Upload & Validation**: Accepts and validates user-uploaded files, storing them temporarily for processing.\n",
        "*    **AI-Powered Metadata Extraction:** Employs an OCR-enhanced metadata generator to extract and analyze text, even from scanned PDFs or images.\n",
        "*   **Smart Analysis:** Produces insights like word/sentence/paragraph counts, content type classification, document summaries, sentiment analysis, and key terms.\n",
        "*  **File Cleanup:** Automatically deletes uploaded files after processing to keep the server clean and efficient.\n",
        "*  **User Interface**: Presents results in a visually appealing web interface with responsive design, keyword tags, and summary boxes.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ETEQ5Z2ZAL-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have to sign in first to get your own ngrok Authenticatication code"
      ],
      "metadata": {
        "id": "SAUBwpJqmoZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import threading\n",
        "import time\n",
        "\n",
        "ngrok.set_auth_token(\"2ypISdRIC82MDPJamtVYjnZFBzP_3vzZnAyRJwMguJzJ1WB6h\")  #i have put my own\n",
        "\n",
        "def run_flask():\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
        "\n",
        "flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
        "flask_thread.start()\n",
        "\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"\\n Public URL: {public_url}\")\n",
        "print(f\" Share this link with anyone!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l79M5SC8RUdf",
        "outputId": "61a486e7-9ef9-4b06-bd0e-c5290efc4274"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Public URL: NgrokTunnel: \"https://c16e-34-168-13-68.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " Share this link with anyone!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* ngrok.set_auth_token(...) sets your personal Ngrok token to authenticate and allow tunnel creation via your Ngrok account.\n",
        "*  **Runs Flask App in Background:** Defines a run_flask() function to start the Flask server locally on port 5000, then starts it in a background thread using Python's threading.\n",
        "*  ngrok.connect(5000) creates a secure public URL that maps to your local Flask app on port 5000. And also displays the public Ngrok link in the console, which you can share to give others access to your local app over the internet.\n"
      ],
      "metadata": {
        "id": "tBebs4_bCKVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill all existing ngrok tunnels\n",
        "try:\n",
        "    ngrok.kill()\n",
        "    print(\" All ngrok tunnels stopped\")\n",
        "except Exception as e:\n",
        "    print(f\" Tunnel cleanup: {e}\")\n",
        "\n",
        "# Wait a moment for cleanup\n",
        "time.sleep(2)\n",
        "\n",
        "# Set your authtoken again\n",
        "ngrok.set_auth_token(\"2ypISdRIC82MDPJamtVYjnZFBzP_3vzZnAyRJwMguJzJ1WB6h\")\n",
        "\n",
        "# Start fresh Flask server on a different port\n",
        "import threading\n",
        "import time\n",
        "from flask import Flask\n",
        "\n",
        "# Create new app instance on different port\n",
        "def run_flask_clean():\n",
        "    try:\n",
        "        print(\" Starting clean Flask server on port 5001...\")\n",
        "        app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Flask error: {e}\")\n",
        "\n",
        "# Start Flask on port 5001 instead of 5000 and wait for flask to start sleep 3 used hence\n",
        "flask_thread = threading.Thread(target=run_flask_clean, daemon=True)\n",
        "flask_thread.start()\n",
        "time.sleep(3)\n",
        "\n",
        "# Create clean tunnel\n",
        "try:\n",
        "    public_url = ngrok.connect(5001)  # Connect to port 5001\n",
        "    print(f\" NEW PUBLIC URL: {public_url}\")\n",
        "    print(f\" Your OCR-enhanced metadata generator is live!\")\n",
        "\n",
        "    print(f\"\\n Open this URL to test your system: {public_url}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error: {e}\")\n",
        "    print(\"\\n Alternative solution:\")\n",
        "    print(\"1. Go to: https://dashboard.ngrok.com/status/tunnels\")\n",
        "    print(\"2. Manually stop all running tunnels\")\n",
        "    print(\"3. Try again with fresh setup\")\n",
        "\n",
        "print(f\"\\n Cleanup and restart complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1v6foiTUUAb",
        "outputId": "07e6adb6-1254-457c-f29d-55c91a2687d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All ngrok tunnels stopped\n",
            " Starting clean Flask server on port 5001...\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5001\n",
            " * Running on http://172.28.0.12:5001\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " NEW PUBLIC URL: NgrokTunnel: \"https://10d5-34-168-13-68.ngrok-free.app\" -> \"http://localhost:5001\"\n",
            " Your OCR-enhanced metadata generator is live!\n",
            "\n",
            " Open this URL to test your system: NgrokTunnel: \"https://10d5-34-168-13-68.ngrok-free.app\" -> \"http://localhost:5001\"\n",
            "\n",
            " Cleanup and restart complete!\n"
          ]
        }
      ]
    }
  ]
}